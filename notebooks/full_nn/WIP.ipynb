{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..\\..')\n",
    "sys.path.insert(0, '../..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'submissions.ab_submission.toolbox' from '/home/ali/git/follicles_detection/notebooks/full_nn/../../submissions/ab_submission/toolbox.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from submissions.full_nn.dataLoaders import imageDataLoader\n",
    "from submissions.full_nn import dataset\n",
    "from submissions.ab_submission import toolbox\n",
    "\n",
    "import importlib\n",
    "\n",
    "importlib.reload(dataset)\n",
    "importlib.reload(toolbox)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import imageio\n",
    "from matplotlib import pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files = glob.glob(\"../../data/train/*.jpg\")\n",
    "test_files = glob.glob(\"../../data/test/*.jpg\")\n",
    "train_label = pd.read_csv(\"../../data/train/labels.csv\")\n",
    "test_label = pd.read_csv(\"../../data/test/labels.csv\")\n",
    "\n",
    "test_loader = imageDataLoader(test_files, test_label[[\"filename\",\"xmin\",\"xmax\",\"ymin\",\"ymax\",\"label\"]])\n",
    "train_loader = imageDataLoader(train_files, train_label[[\"filename\",\"xmin\",\"xmax\",\"ymin\",\"ymax\",\"label\"]])\n",
    "predict_loader = imageDataLoader(test_files, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch import nn\n",
    "from torchvision.transforms import Resize, RandomHorizontalFlip, RandomVerticalFlip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader_collapse (x, resize_ratio=1, random_flip=True):\n",
    "    \"\"\"dataloader_collate\n",
    "    \n",
    "    Function of collate for the DataLoader class use with the datasetFollicles class.\n",
    "    This function is used to provide images from the datasetFollicles in a tensor for training task.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x: input data to collate, list containing tuple of a numpy array of size (h,w,3) and a dictionnary is expected\n",
    "    random_flip: boolean, if True, random flip of the images are performed\n",
    "\n",
    "    Output\n",
    "    ------\n",
    "    Tuple containing a tensor of size :\n",
    "        (batch_size, *features) with features the size of the features data\n",
    "        (batch_size, 5) with 5 the one-hot encoding of the label\n",
    "    \"\"\"\n",
    "\n",
    "    Xs = []\n",
    "    ys = []\n",
    "\n",
    "    # Compute resize height and width\n",
    "    height = int(x[0][1][\"height\"]/resize_ratio)\n",
    "    width = int(x[0][1][\"width\"]/resize_ratio)\n",
    "    h_ratio = height/x[0][1][\"height\"]\n",
    "    w_ratio = width/x[0][1][\"width\"]\n",
    "\n",
    "    resizer = Resize((height, width))\n",
    "\n",
    "    if random_flip:\n",
    "        random_hflip = RandomHorizontalFlip(p=1)\n",
    "        random_vflip = RandomVerticalFlip(p=1)\n",
    "\n",
    "    for data in x:\n",
    "        # Getting data in tensor\n",
    "        data_tensor = torch.tensor(data[0], dtype=torch.float32)\n",
    "        data_tensor = torch.moveaxis(data_tensor, 2, 0)\n",
    "\n",
    "        # Resize image\n",
    "        data_tensor = resizer(data_tensor)\n",
    "\n",
    "        # Random transformations\n",
    "        if random_flip:\n",
    "            random_flip_number = (np.random.random(2) <= 0.5)\n",
    "            if random_flip_number[0]:\n",
    "                data_tensor = random_hflip(data_tensor)\n",
    "            if random_flip_number[1]:\n",
    "                data_tensor = random_vflip(data_tensor)\n",
    "\n",
    "        # Getting label and box\n",
    "\n",
    "        if data[1][\"bbox_label\"] is not None:\n",
    "            ## Computing box\n",
    "            bbox = np.array(data[1][\"bbox\"])\n",
    "            bbox[:,0], bbox[:,2] = bbox[:,0]*w_ratio, bbox[:,2]*w_ratio\n",
    "            bbox[:,1], bbox[:,3] = bbox[:,1]*h_ratio, bbox[:,3]*h_ratio\n",
    "            bbox = np.floor(bbox).astype(\"int\")\n",
    "\n",
    "            ## Fixing box according to flip\n",
    "            if random_flip:\n",
    "                if random_flip_number[0]:\n",
    "                    bbox[:,0], bbox[:,2] = width-bbox[:,2], width-bbox[:,0]\n",
    "                if random_flip_number[1]:\n",
    "                    bbox[:,1], bbox[:,3] = height-bbox[:,3], height-bbox[:,1]\n",
    "\n",
    "            ## Removing negative labels\n",
    "            label_mask = (np.array(data[1][\"bbox_label\"]) != 0)\n",
    "\n",
    "            ## Removing empty box\n",
    "            not_empty_mask = ((bbox[:,2]-bbox[:,0])*(bbox[:,3]-bbox[:,1])) != 0\n",
    "\n",
    "            bbox_label = {\n",
    "                    \"boxes\":torch.tensor(bbox, dtype=torch.int64)[(label_mask) & (not_empty_mask)],\n",
    "                    \"labels\":torch.tensor(data[1][\"bbox_label\"], dtype=torch.int64)[(label_mask) & (not_empty_mask)]\n",
    "            }\n",
    "\n",
    "            ys.append(bbox_label)\n",
    "        else:\n",
    "            y = None\n",
    "        \n",
    "        Xs.append(data_tensor)\n",
    "\n",
    "    X = torch.stack(Xs)\n",
    "    y = ys\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection import fasterrcnn_mobilenet_v3_large_320_fpn, fasterrcnn_resnet50_fpn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset.folliclesDataset(\n",
    "    train_loader,\n",
    "    local_path=\"../../data/train_dataset_fullnn\",\n",
    "    window_size=(1000,1000),\n",
    "    verbose=False,\n",
    "    force_reload=False\n",
    ")\n",
    "\n",
    "test_dataset = dataset.folliclesDataset(\n",
    "    test_loader,\n",
    "    local_path=\"../../data/test_dataset_fullnn\",\n",
    "    window_size=(1000,1000),\n",
    "    verbose=False,\n",
    "    border_condition=\"ignore\",\n",
    "    force_reload=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize_ratio=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "collater = lambda x: dataloader_collapse(x, resize_ratio=resize_ratio, random_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generatePrediction (filename, dataset, resize_factor=4):\n",
    "\n",
    "    # Getting data\n",
    "    data = dataset[filename]\n",
    "\n",
    "    # Resizing\n",
    "    data_resized = []\n",
    "    data_resized_ratio = []\n",
    "    data_metadata = []\n",
    "\n",
    "    for x in data:\n",
    "        x_ = torch.tensor(x[0], dtype=torch.float32)/255.\n",
    "\n",
    "        width = int(x_.shape[0]/resize_factor)\n",
    "        height = int(x_.shape[1]/resize_factor)\n",
    "        resizer = Resize((height, width))\n",
    "\n",
    "        width_ratio = x_.shape[0]/width\n",
    "        height_ratio = x_.shape[1]/height\n",
    "\n",
    "        x_ = torch.moveaxis(x_, 2, 0)\n",
    "        x_ = resizer(x_)\n",
    "\n",
    "        data_resized.append(x_)\n",
    "        data_resized_ratio.append((width_ratio, height_ratio))\n",
    "\n",
    "        data_metadata.append(x[1])\n",
    "    \n",
    "    return data_resized, data_resized_ratio, data_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collater)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fasterrcnn_mobilenet_v3_large_320_fpn(pretrained=True)\n",
    "\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, 5)\n",
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.Adam(params, lr=1e-5)\n",
    "\n",
    "model = model.to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Piste\n",
    "## Intesect Moving window\n",
    "## Pretrain breast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] loss_classifier : 0.1470124568603933 - loss_box_reg : 0.15043586176587267 - loss_objectness : 0.011206137056287844 - loss_rpn_box_reg : 0.002899003131024074\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "print_epoch = 1\n",
    "print_iter_freq = 100\n",
    "\n",
    "losses_history = []\n",
    "\n",
    "for i in range(n_epochs):\n",
    "\n",
    "    if (i%print_epoch == 0) and (i != 0):\n",
    "        loss_mean = np.array(epoch_loss).mean(axis=0)\n",
    "        print(f\"Epoch n°{i} - loss_classifier : {loss_mean[0]} - loss_box_reg : {loss_mean[1]} - loss_objectness : {loss_mean[2]} - loss_rpn_box_reg : {loss_mean[3]}\")\n",
    "\n",
    "    if i != 0:\n",
    "        losses_history += epoch_loss\n",
    "    \n",
    "    epoch_loss = []\n",
    "\n",
    "    iters = 0\n",
    "    for x, y in train_dataloader:\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if (iters%print_iter_freq == 0) and (iters != 0):\n",
    "            loss_mean = np.array(epoch_loss).mean(axis=0)\n",
    "            print(f\"[{i}] loss_classifier : {loss_mean[0]} - loss_box_reg : {loss_mean[1]} - loss_objectness : {loss_mean[2]} - loss_rpn_box_reg : {loss_mean[3]}\")\n",
    "\n",
    "        x = x.to(\"cuda:0\")\n",
    "        x = x/255.\n",
    "        for j in range(len(y)):\n",
    "            for key in y[j].keys():\n",
    "                y[j][key] = y[j][key].to(\"cuda:0\")\n",
    "\n",
    "        loss = model(x,y)\n",
    "        \n",
    "        losses = sum(loss_ for loss_ in loss.values())\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss.append([x.detach().cpu().item() for x in loss.values()])\n",
    "\n",
    "        iters += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    res = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image_torch(x):\n",
    "    x_ = x.detach().cpu()\n",
    "    x_ = torch.moveaxis(x, 0, 2)\n",
    "    x_ = x_.cpu().numpy()\n",
    "    x_ = x_.astype(\"int\")\n",
    "\n",
    "    plt.imshow(x_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = test_loader\n",
    "dataset_ = test_dataset\n",
    "\n",
    "files = loader.X_filenames\n",
    "\n",
    "id_to_class = {\n",
    "    1:\"Primordial\",\n",
    "    2:\"Primaire\",\n",
    "    3:\"Secondaire\",\n",
    "    4:\"Tertiaire\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "\n",
    "    predictions = {}\n",
    "    for filename in files:\n",
    "        data = generatePrediction(filename, dataset_, resize_factor=resize_ratio)\n",
    "\n",
    "        predictions[filename] = []\n",
    "        for x in data[0]:\n",
    "            x = x.unsqueeze(dim=0).to(\"cuda:0\")\n",
    "            y_hat = model(x)[0]\n",
    "\n",
    "            for key in y_hat.keys():\n",
    "                y_hat[key] = y_hat[key].detach().cpu().tolist()\n",
    "\n",
    "            predictions[filename].append(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.ops import box_iou, nms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing filename D-1M06-4.jpg\n",
      "Parsing filename D-1M06-2.jpg\n",
      "Parsing filename D-1M06-5.jpg\n",
      "Parsing filename D-1M06-1.jpg\n",
      "Parsing filename D-1M06-3.jpg\n"
     ]
    }
   ],
   "source": [
    "for filename in files:\n",
    "    image_prediction = []\n",
    "    image_labels = []\n",
    "    \n",
    "    print(f\"Parsing filename {filename}\")\n",
    "\n",
    "    data = generatePrediction(filename, dataset_, resize_factor=resize_ratio)\n",
    "    for window, resize_ratio, metadata, pred in zip(*(data + (predictions[filename],))):\n",
    "        if len(pred[\"boxes\"]) > 0:\n",
    "            # Getting correct box offsets\n",
    "            pred_array = np.array(pred[\"boxes\"])\n",
    "            pred_array[:,0], pred_array[:,2] = resize_ratio[0]*pred_array[:,0], resize_ratio[0]*pred_array[:,2]\n",
    "            pred_array[:,1], pred_array[:,3] = resize_ratio[1]*pred_array[:,1], resize_ratio[1]*pred_array[:,3]\n",
    "            \n",
    "            window_offset_mask = np.repeat([metadata[\"window_offsets\"][0:2]],2, axis=0).flatten()\n",
    "            pred_array = (window_offset_mask+pred_array).astype(\"int\")\n",
    "            \n",
    "            # Merging\n",
    "            pred_boxes = torch.tensor(pred_array, dtype=torch.float32)\n",
    "            pred_scores = torch.tensor(pred[\"scores\"], dtype=torch.float32)\n",
    "            pred_labels = torch.tensor(pred[\"labels\"], dtype=torch.int8)\n",
    "            indices_nms = nms(\n",
    "                boxes=pred_boxes,\n",
    "                scores=pred_scores,\n",
    "                iou_threshold=0.7\n",
    "            ) # Check if it works\n",
    "\n",
    "            pred_boxes = pred_boxes.int()\n",
    "\n",
    "            image_prediction += pred_boxes[indices_nms,:].tolist()\n",
    "            image_labels += pred_labels[indices_nms].tolist()\n",
    "\n",
    "    # Getting image\n",
    "    preds = [{\"bbox\":x, \"class\":id_to_class[y]} for x,y in zip(image_prediction, image_labels)]\n",
    "    image = loader.get_sample(filename)[0]\n",
    "\n",
    "    toolbox.write_rectangle(\n",
    "        image,\n",
    "        preds,\n",
    "        folder=\"../../data/test_predicted_fullnn\",\n",
    "        filename=filename\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "28b293e0c0671e44c7281dde6399c7c7419d3faca031d22494da8635907ada72"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
