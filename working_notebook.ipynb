{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from submissions.ab_submission.object_detector import imageDataLoader, follicleClassifier, ObjectDetector\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "train_files = glob.glob(\"./data/train/*.jpg\")\n",
    "test_files = glob.glob(\"./data/test/*.jpg\")\n",
    "train_label = pd.read_csv(\"./data/train/labels.csv\")\n",
    "test_label = pd.read_csv(\"./data/test/labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from ./params/follicleClassifier2.model\n",
      "Fitting boxPixelClassifier\n",
      "Fitting follicleClassifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\4078182\\Miniconda3\\lib\\site-packages\\sklearn\\base.py:324: UserWarning: Trying to unpickle estimator DecisionTreeClassifier from version 0.24.2 when using version 1.0.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<submissions.ab_submission.object_detector.ObjectDetector at 0x27fa7aedb80>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ObjectDetector(ramp_mode=False)\n",
    "model.load(boxPixelClassifier=\"./params/boxPixelClassifier_opencv.joblib\", follicleClassifier=\"./params/follicleClassifier2.model\")\n",
    "model.fit(train_files, train_label)\n",
    "#model.save(boxPixelClassifier=\"./params/boxPixelClassifier_opencv.joblib\",follicleClassifier=\"./params/follicleClassifier2.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = imageDataLoader(test_files, test_label[[\"filename\",\"xmin\",\"xmax\",\"ymin\",\"ymax\",\"label\"]])\n",
    "train_loader = imageDataLoader(train_files, train_label[[\"filename\",\"xmin\",\"xmax\",\"ymin\",\"ymax\",\"label\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model.predict(test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "font_size = 60\n",
    "if os.name != 'nt':\n",
    "    font = ImageFont.truetype(\"/usr/share/fonts/truetype/freefont/FreeMono.ttf\", font_size)\n",
    "else:\n",
    "    font = ImageFont.truetype(\"C:/Windows/Fonts/Arial/arialbd.ttf\", font_size)\n",
    "\n",
    "def write_rectangle(image, preds, folder=None, filename=None):\n",
    "    img = Image.fromarray(image)\n",
    "    img_draw = ImageDraw.Draw(img)\n",
    "    for pred in preds:\n",
    "        x1, y1, x2, y2 = pred[\"bbox\"]\n",
    "        label = pred[\"class\"]\n",
    "        img_draw.rounded_rectangle(((x1, y1), (x2,y2)), fill=None, outline=\"black\", width=5)\n",
    "        img_draw.text((x1, y1-70), label, font=font, fill=\"black\")\n",
    "\n",
    "    if folder is not None and filename is not None:\n",
    "        img.save(f\"./data/{folder}/{filename}\")\n",
    "    \n",
    "    return np.array(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "folder = \"test_predicted\"\n",
    "for x in test_loader.get_samples():\n",
    "    write_rectangle(x[0], y_hat[i], folder=folder, filename=test_loader.X_filenames[i])\n",
    "    i += 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempFolder = tempfile.TemporaryFile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class folliclesDataset(Dataset):\n",
    "    \"\"\"folliclesDataset\n",
    "    \n",
    "    This class provide a dataset for follicles algorithm training.\n",
    "    The aim is to perform all the data transform and augmentation at the same place.\n",
    "    This class provide an iterate, either it provide data in live, either it stores them in hard drive and provide them from memory \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__ (self, image_loader, data_augmentation, local_path, box_classifier = None, verbose=True):\n",
    "        \"\"\"Parameters\n",
    "            ----------\n",
    "            image_loader: object from the image loader class\n",
    "            data_augmentation: boolean, if True a data augmentation is performed\n",
    "            local_path: str, local path for data storage which are kept in memory as picke in the local_path folder\n",
    "            box_classifier: object from the box classifier class, if None no box are generated from the classifier\n",
    "            verbose: boolean, informations about current operations are displayed\n",
    "        \"\"\"\n",
    "\n",
    "        # Storing the image loader\n",
    "        self.image_loader = image_loader\n",
    "        self.box_classifier = box_classifier\n",
    "\n",
    "        # Storing the parameters\n",
    "        if local_path is not None and os.path.exists(local_path):\n",
    "            self.local_path = local_path\n",
    "            self.store_data = True\n",
    "        else:\n",
    "            raise Exception(\"The provided path doesn't exist.\")\n",
    "\n",
    "        self.data_augmentation = data_augmentation\n",
    "        self.verbose = verbose\n",
    "\n",
    "        # Recording metadata\n",
    "        ## Contains the dataset metadata\n",
    "        ## files metadata, files location\n",
    "        self.metadata = []\n",
    "\n",
    "        # Generating data\n",
    "        if self.verbose:\n",
    "            print(\"Generating data\")\n",
    "            self._generate_all_data()\n",
    "\n",
    "    def _generate_all_data(self, label_ratio_threshold=0.7):\n",
    "        \"\"\"Function that generate all the data\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        label_ratio_threshold: threshold of percentage of box intersection for keeping it\n",
    "        \"\"\"\n",
    "\n",
    "        for filename in self.X_filenames:\n",
    "            output_data = self._generate_data(filename)\n",
    "            output_dataset = [x[\"data\"] for x in output_data]\n",
    "            output_metadata = [dict([(key,value) for key, value in x.items() if key not in [\"data\"]]) for x in output_data]\n",
    "            output_filenames = for x in range(len(self.metadata)-1, len(self.metadata)+len(output_metadata)-1)\n",
    "\n",
    "    def _generate_data(self, filename, label_ratio_threshold=0.7):\n",
    "        \"\"\"Generate the data from a sample\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        filename: str, name of the file from which we generate the data\n",
    "        label_ratio_threshold: threshold of percentage of box intersection for keeping it\n",
    "\n",
    "        Output\n",
    "        ------\n",
    "        List of dict, containing :\n",
    "            filename: name of the original file\n",
    "            width: width of the box\n",
    "            height: height of the box\n",
    "            ratio: ratio h/w of the box\n",
    "            bbox: xmin, ymin, xmax, ymax of the box\n",
    "            data: box content\n",
    "            label: label of the box \n",
    "        \"\"\"\n",
    "\n",
    "        # Getting original data and cropped data\n",
    "        original_data = self.image_loader.get_sample(filename)\n",
    "        original_image, original_boxes, original_labels = original_data[0], original_data[1], original_data[2]\n",
    "        original_image_shape = original_data[-1]\n",
    "        original_image_crop = self.image_loader.get_crop(original_image, original_boxes, data_augmentation=self.data_augmentation)\n",
    "\n",
    "        # Getting the box\n",
    "        detected_box = self.box_classifier(image_loader = self.image_loader, image_name = self.filename)\n",
    "\n",
    "        # Filter boxs and get labels\n",
    "        new_box_coordonates, new_box_data, new_box_labels = self._filter_box(original_image=original_image, \n",
    "                                                                            original_boxes=original_boxes, \n",
    "                                                                            original_labels=original_labels,\n",
    "                                                                            detected_box=detected_box,\n",
    "                                                                            label_ratio_threshold=label_ratio_threshold\n",
    "                                                            )\n",
    "        \n",
    "        # Creating the output data\n",
    "        output_dict = []\n",
    "        \n",
    "        ## From original data\n",
    "\n",
    "        output_data = [\n",
    "            zip(original_boxes, original_image_crop, original_labels),\n",
    "            zip(new_box_coordonates, new_box_data, new_box_labels)\n",
    "        ]\n",
    "        output_dict += [{\n",
    "            \"filename\":filename,\n",
    "            \"height\":original_image_shape[0],\n",
    "            \"width\":original_image_shape[1],\n",
    "            \"ratio\":original_image_shape[0]/original_image_shape[1],\n",
    "            \"bbox\":x[0],\n",
    "            \"data\":x[1],\n",
    "            \"label\":x[2]\n",
    "        } for data in output_data for x in data]\n",
    "\n",
    "        return output_dict\n",
    "        \n",
    "\n",
    "    def _filter_box(self, original_image, original_boxes, original_labels, detected_box, label_ratio_threshold=0.7):\n",
    "        \"\"\"Given a box list, return a filtered list and its labels\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        original_image: numpy array of size (h, w) of the original image\n",
    "        original_boxes: list of original box locations in format xmin, xmax, ymin, ymax\n",
    "        original_labels: list integer corresponding of the labels of the original box\n",
    "        detected_box: list of detected box in formay xmin, ymin, xmax, ymax\n",
    "        label_ratio_threshold: threshold of percentage of box intersection for keeping it\n",
    "\n",
    "        Output\n",
    "        ------\n",
    "        Tuple new_box_coordonates, new_box_data, new_box_label :\n",
    "        - new_box_coordonates: list of xmin, ymin, xmax and ymax coordonates\n",
    "        - new_box_data: numpy array of size (h,w) which contains the content of the box\n",
    "        - new_box_label: int of the box class\n",
    "        \"\"\"\n",
    "\n",
    "        # We create a reference matrix, which contains the true labels\n",
    "        label_matrix = np.ones(original_image[0:2])*-1\n",
    "        for original_box, original_label in zip(original_boxes, original_labels):\n",
    "            label_matrix[original_box[2]:original_box[3],original_box[0]:original_box[1]] = original_label\n",
    "\n",
    "        new_box_coordonates = []\n",
    "        new_box_data = []\n",
    "        new_box_label = []\n",
    "\n",
    "        for box in detected_box:\n",
    "            # Create a temporary matrix for working on data\n",
    "            working_matrix = label_matrix[box[1]:box[3],box[0]:box[2]]\n",
    "            if np.max(working_matrix) != -1:      \n",
    "                # Compute the proportion of pixels with a label\n",
    "                label_ratio = (working_matrix != -1).mean()\n",
    "                \n",
    "                if label_ratio > label_ratio_threshold:\n",
    "                    box_label = np.argmax(np.bincount(working_matrix[working_matrix != 0].astype(\"int8\")))-1\n",
    "                    box_data = original_image[box[1]:box[3], box[0]:box[2]]\n",
    "\n",
    "                    new_box_coordonates.append(box)\n",
    "                    new_box_data.append(box_data)\n",
    "                    new_box_label.append(box_label)\n",
    "\n",
    "        return new_box_coordonates, new_box_data, new_box_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageDataLoader.get_sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Original image\n",
    "    original_data = image_loader.get_sample(filename)\n",
    "    original_image_crop = image_loader.get_crop(original_data[0], original_data[1])\n",
    "\n",
    "    ## Dataset from original data\n",
    "    dataset_for_follicle_classifier_original = [original_image_crop, original_data[2]]\n",
    "\n",
    "    ##  Dataset from boxes\n",
    "    ### Here we compute a matrix of zeros with the location of original labelled data\n",
    "    ### We only keep box which intersect with theses\n",
    "    boxes = self._get_box_list(image_loader=image_loader, image_name=filename)\n",
    "\n",
    "    label_matrix = np.zeros(original_data[-1][0:2])\n",
    "    for original_box, original_label in zip(original_data[1], original_data[2]):\n",
    "        label_matrix[original_box[2]:original_box[3],original_box[0]:original_box[1]] = original_label+1\n",
    "\n",
    "    dataset_for_follicle_classifier_box_data = []\n",
    "    dataset_for_follicle_classifier_box_label = []\n",
    "    for box in boxes:\n",
    "        tmp_matrix = label_matrix[box[1]:box[3],box[0]:box[2]]\n",
    "        if np.max(tmp_matrix) != 0:        \n",
    "            area = tmp_matrix.shape[0]*tmp_matrix.shape[1]\n",
    "            n_pixels = (tmp_matrix != 0).sum()\n",
    "            \n",
    "            if area/n_pixels > 0.5:\n",
    "                box_label = np.argmax(np.bincount(tmp_matrix[tmp_matrix != 0].astype(\"int8\")))-1\n",
    "                box_data = original_data[0][box[1]:box[3], box[0]:box[2]]\n",
    "\n",
    "                dataset_for_follicle_classifier_box_data.append(box_data)\n",
    "                dataset_for_follicle_classifier_box_label.append(box_label)\n",
    "\n",
    "    if len(dataset_for_follicle_classifier) != 0:\n",
    "        dataset_for_follicle_classifier.append(\n",
    "            dataset_for_follicle_classifier_original\n",
    "        )\n",
    "\n",
    "    if len(dataset_for_follicle_classifier_box_data) != 0:\n",
    "        dataset_for_follicle_classifier.append(\n",
    "            [dataset_for_follicle_classifier_box_data, dataset_for_follicle_classifier_box_label]\n",
    "        )\n",
    "\n",
    "tensors_for_follicle_classifier = []\n",
    "\n",
    "for data in dataset_for_follicle_classifier:\n",
    "    x = [cv2.resize(\n",
    "            image, \n",
    "            (self.follicle_classifier_size,\n",
    "            int(image.shape[0]*self.follicle_classifier_size/image.shape[1]))\n",
    "        ) for image in data[0] if len(image.shape) == 3]\n",
    "\n",
    "    # Padding to get a dataset of same size everywhere\n",
    "    if len(x) > 0:\n",
    "        x = nn.utils.rnn.pad_sequence([torch.tensor(data, dtype=torch.float32) for data in x], batch_first=True)\n",
    "        x = torch.moveaxis(x, 3, 1)\n",
    "        x = x/255 # VGG requires a normalized pixel intensity\n",
    "\n",
    "        # One hot encoding of the labels\n",
    "        y = nn.functional.one_hot(\n",
    "            torch.tensor(data[1], dtype=torch.int64), \n",
    "            num_classes=5\n",
    "        ).float()\n",
    "\n",
    "        tensors_for_follicle_classifier.append((x,y))\n",
    "\n",
    "for i in range(self.n_epochs):\n",
    "    for x,y in tensors_for_follicle_classifier:\n",
    "        n_batch = x.shape[0]//self.batch_size + int(x.shape[0]%self.batch_size)\n",
    "        for batch in range(n_batch):\n",
    "            x_temp = x[batch*self.batch_size:(batch+1)*self.batch_size].to(self.device)\n",
    "            y_temp = y[batch*self.batch_size:(batch+1)*self.batch_size].to(self.device)\n",
    "            \n",
    "            if x_temp.shape[0] > 1:\n",
    "                self.follicleClassifier.fit(x_temp,y_temp)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "28b293e0c0671e44c7281dde6399c7c7419d3faca031d22494da8635907ada72"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
