{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from submissions.ab_submission.dataLoaders import imageDataLoader\n",
    "from submissions.ab_submission.follicleClassifier import follicleClassifier\n",
    "from submissions.ab_submission.object_detector import ObjectDetector\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "train_files = glob.glob(\"./data/train/*.jpg\")\n",
    "test_files = glob.glob(\"./data/test/*.jpg\")\n",
    "train_label = pd.read_csv(\"./data/train/labels.csv\")\n",
    "test_label = pd.read_csv(\"./data/test/labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from ./params/follicleClassifier2.model\n",
      "Fitting boxPixelClassifier\n",
      "Fitting follicleClassifier\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<submissions.ab_submission.object_detector.ObjectDetector at 0x7f9b035be4f0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ObjectDetector(ramp_mode=False)\n",
    "model.load(boxPixelClassifier=\"./params/boxPixelClassifier_opencv.joblib\", follicleClassifier=\"./params/follicleClassifier2.model\")\n",
    "model.fit(train_files, train_label)\n",
    "#model.save(boxPixelClassifier=\"./params/boxPixelClassifier_opencv.joblib\",follicleClassifier=\"./params/follicleClassifier2.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = imageDataLoader(test_files, test_label[[\"filename\",\"xmin\",\"xmax\",\"ymin\",\"ymax\",\"label\"]])\n",
    "train_loader = imageDataLoader(train_files, train_label[[\"filename\",\"xmin\",\"xmax\",\"ymin\",\"ymax\",\"label\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_474065/3261902781.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/git/follicles_detection/submissions/ab_submission/object_detector.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m             \u001b[0;31m# Getting box location\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m             \u001b[0my_hat_box\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_box_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0;31m# Classification of the box\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/follicles_detection/submissions/ab_submission/object_detector.py\u001b[0m in \u001b[0;36m_get_box_list\u001b[0;34m(self, image_loader, image_name)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;31m# Data for detection of pixel of interest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         pixel_data = image_loader.get_pixel_labels_sample(\n\u001b[0m\u001b[1;32m    133\u001b[0m             \u001b[0mimage_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0mpre_processing_fullimage_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_preprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/follicles_detection/submissions/ab_submission/dataLoaders.py\u001b[0m in \u001b[0;36mget_pixel_labels_sample\u001b[0;34m(self, image_name, pre_processing_fullimage_func, normalize, random_pick, random_state, all)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0;31m# Normalization of the image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m             \u001b[0mimage_data_normalized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_normalize_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_data_box\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m             \u001b[0;31m# Then we flatten the array according to each color channel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/follicles_detection/submissions/ab_submission/dataLoaders.py\u001b[0m in \u001b[0;36m_normalize_image\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0;31m# The idea behind this is to normalize the image according to itself, we want to make sure that it's pixel intensity is not \"shift\" and is always between 0 and 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m         \u001b[0mimage_normalized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m         \u001b[0mimage_normalized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimage_normalized\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mimage_normalized\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/lib/python3.9/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_amax\u001b[0;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m     38\u001b[0m def _amax(a, axis=None, out=None, keepdims=False,\n\u001b[1;32m     39\u001b[0m           initial=_NoValue, where=True):\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mumr_maximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m def _amin(a, axis=None, out=None, keepdims=False,\n",
      "\u001b[0;32m~/.miniconda3/lib/python3.9/site-packages/imageio/core/util.py\u001b[0m in \u001b[0;36m__array_wrap__\u001b[0;34m(self, out, context)\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copy_meta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__array_wrap__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \"\"\" So that we return a native numpy array (or scalar) when a\n\u001b[1;32m    162\u001b[0m         \u001b[0mreducting\u001b[0m \u001b[0mufunc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mapplied\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msuch\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "y_hat = model.predict(test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "font_size = 60\n",
    "if os.name != 'nt':\n",
    "    font = ImageFont.truetype(\"/usr/share/fonts/truetype/freefont/FreeMono.ttf\", font_size)\n",
    "else:\n",
    "    font = ImageFont.truetype(\"C:/Windows/Fonts/Arial/arialbd.ttf\", font_size)\n",
    "\n",
    "def write_rectangle(image, preds, folder=None, filename=None):\n",
    "    img = Image.fromarray(image)\n",
    "    img_draw = ImageDraw.Draw(img)\n",
    "    for pred in preds:\n",
    "        x1, y1, x2, y2 = pred[\"bbox\"]\n",
    "        label = pred[\"class\"]\n",
    "        img_draw.rounded_rectangle(((x1, y1), (x2,y2)), fill=None, outline=\"black\", width=5)\n",
    "        img_draw.text((x1, y1-70), label, font=font, fill=\"black\")\n",
    "\n",
    "    if folder is not None and filename is not None:\n",
    "        img.save(f\"./data/{folder}/{filename}\")\n",
    "    \n",
    "    return np.array(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_hat' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_451454/2888743898.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfolder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"test_predicted\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mwrite_rectangle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_hat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_filenames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_hat' is not defined"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "folder = \"test_predicted\"\n",
    "for x in test_loader.get_samples():\n",
    "    write_rectangle(x[0], y_hat[i], folder=folder, filename=test_loader.X_filenames[i])\n",
    "    i += 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import tempfile\n",
    "import pickle\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempFolder_test = tempfile.TemporaryDirectory()\n",
    "tempFolder_train = tempfile.TemporaryDirectory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (1175448563.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_476283/1175448563.py\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    tensor_reducer: str, \"pad\" if image are padded, \"resize\" if the are all resized in the same size\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "        tensor_reducer=\"pad\",\n",
    "        tensor_reducer_max_height=128\n",
    "            tensor_reducer: str, \"pad\" if image are padded, \"resize\" if the are all resized in the same size \n",
    "            tensor_reducer_max_height: int, max height of the image during the tensor generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class folliclesDataset(Dataset):\n",
    "    \"\"\"folliclesDataset\n",
    "    \n",
    "    This class provide a dataset for follicles algorithm training.\n",
    "    The aim is to perform all the data transform and augmentation at the same place.\n",
    "    This class provide an iterate, either it provide data in live, either it stores them in hard drive and provide them from memory \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__ (self, \n",
    "        image_loader, \n",
    "        data_augmentation, \n",
    "        local_path, \n",
    "        box_classifier = None, \n",
    "        verbose=True, \n",
    "        force_reload=False,\n",
    "        order=\"default\"\n",
    "    ):\n",
    "        \"\"\"Parameters\n",
    "            ----------\n",
    "            image_loader: object from the image loader class\n",
    "            data_augmentation: boolean, if True a data augmentation is performed\n",
    "            local_path: str, local path for data storage which are kept in memory as picke in the local_path folder\n",
    "            box_classifier: object from the box classifier class, if None no box are generated from the classifier\n",
    "            verbose: boolean, informations about current operations are displayed\n",
    "            force_reload: boolean, if the data have already been load for a given folder, they are not loaded again if force_reload is set at False\n",
    "            order: str, ('default','box_ratio') : order in which the data would be iterate with __get_item___, by default it is the order of the data, if box_ratio it is the ratio of each box\n",
    "        \"\"\"\n",
    "\n",
    "        # Storing the image loader\n",
    "        self.image_loader = image_loader\n",
    "        self.box_classifier = box_classifier\n",
    "\n",
    "        # Storing the parameters\n",
    "        if local_path is not None and os.path.exists(local_path):\n",
    "            self.local_path = local_path\n",
    "        else:\n",
    "            raise Exception(\"The provided path doesn't exist.\")\n",
    "\n",
    "        self.data_augmentation = data_augmentation\n",
    "        self.verbose = verbose\n",
    "\n",
    "        # Recording metadata\n",
    "        ## Contains the dataset metadata\n",
    "        ## files metadata, files location\n",
    "        self.metadata_path = \"/\".join([\n",
    "            self.local_path,\n",
    "            \"metadata.pickle\"\n",
    "        ])\n",
    "        self.metadata = []\n",
    "        load_data = True\n",
    "\n",
    "        # Checking is metadata have been already loaded\n",
    "        if os.path.isfile(self.metadata_path):\n",
    "            with open(self.metadata_path, \"rb\") as f:\n",
    "                self.metadata = pickle.load(f)\n",
    "\n",
    "                if force_reload == False:\n",
    "                    load_data = False\n",
    "\n",
    "        # Generating data\n",
    "        if load_data:\n",
    "            if self.verbose:\n",
    "                print(\"Generating data\")\n",
    "\n",
    "            self._generate_all_data()\n",
    "\n",
    "            self._write_metadata()\n",
    "\n",
    "        self.set_order(order=order)\n",
    "\n",
    "    def set_order(self, order):\n",
    "        \"\"\"set_order\n",
    "\n",
    "        Calling this function change temporary the order of the data\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        order: str, ('default','box_ratio') : order in which the data would be iterate with __get_item___, by default it is the order of the data, if box_ratio it is the ratio of each box\n",
    "        \"\"\"\n",
    "\n",
    "        if order in (\"default\",'box_ratio'):\n",
    "            metadata_range = list(range(len(self.metadata)))\n",
    "            if order==\"default\":\n",
    "                self.metadata_mask = dict(zip(\n",
    "                    metadata_range,\n",
    "                    metadata_range\n",
    "                ))\n",
    "            elif order=='box_ratio':\n",
    "                ratio_list = [x[\"ratio\"] for x in self.metadata]\n",
    "                ratio_sorted_list = np.argsort(ratio_list).tolist()\n",
    "                self.metadata_mask = dict(zip(\n",
    "                    metadata_range,\n",
    "                    ratio_sorted_list\n",
    "                ))\n",
    "                \n",
    "\n",
    "    def _generate_all_data(self, label_ratio_threshold=0.7):\n",
    "        \"\"\"Function that generate and write all the data\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        label_ratio_threshold: threshold of percentage of box intersection for keeping it\n",
    "\n",
    "        Output\n",
    "        ------\n",
    "        No output. It writes all the data.\n",
    "        \"\"\"\n",
    "\n",
    "        for filename in self.image_loader.X_filenames:\n",
    "            output_data = self._generate_data(filename)\n",
    "\n",
    "            output_filenames = [\n",
    "                \"/\".join([\n",
    "                    self.local_path,\n",
    "                    str(x)\n",
    "                ])+\".pickle\" for x in range(\n",
    "                    len(self.metadata), \n",
    "                    len(self.metadata)+len(output_data)\n",
    "            )]\n",
    "\n",
    "            for data, filename in zip(output_data, output_filenames):\n",
    "                if self.verbose:\n",
    "                    print(f\"Writting {filename}\")\n",
    "\n",
    "                output_dict = dict([(key,value) for key, value in data.items() if key not in [\"data\"]])\n",
    "                output_dict[\"path\"] = filename\n",
    "\n",
    "                # Keeping the data in the internal metadata list\n",
    "                self.metadata.append(output_dict)\n",
    "                # Writting file\n",
    "                with open(filename,\"wb\") as f:\n",
    "                    pickle.dump(data[\"data\"], f)\n",
    "\n",
    "\n",
    "    def _generate_data(self, filename, label_ratio_threshold=0.7):\n",
    "        \"\"\"Generate the data from a sample\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        filename: str, name of the file from which we generate the data\n",
    "        label_ratio_threshold: threshold of percentage of box intersection for keeping it\n",
    "\n",
    "        Output\n",
    "        ------\n",
    "        List of dict, containing :\n",
    "            filename: name of the original file\n",
    "            status: if data from original crop or not\n",
    "            width: width of the box\n",
    "            height: height of the box\n",
    "            ratio: ratio h/w of the box\n",
    "            bbox: xmin, ymin, xmax, ymax of the box\n",
    "            data: box content\n",
    "            label: label of the box \n",
    "        \"\"\"\n",
    "\n",
    "        # Getting original data and cropped data\n",
    "        original_data = self.image_loader.get_sample(filename)\n",
    "        original_image, original_boxes, original_labels = original_data[0], original_data[1], original_data[2]\n",
    "        original_image_shape = original_data[-1]\n",
    "        original_image_crop = self.image_loader.get_crop(original_image, original_boxes, image_labels=original_labels, data_augmentation=self.data_augmentation)\n",
    "\n",
    "        # Getting the box\n",
    "        detected_box = self.box_classifier(image_loader = self.image_loader, image_name = filename)\n",
    "\n",
    "        # Filter boxs and get labels\n",
    "        new_box_coordonates, new_box_data, new_box_labels = self._filter_box(original_image=original_image, \n",
    "                                                                            original_boxes=original_boxes, \n",
    "                                                                            original_labels=original_labels,\n",
    "                                                                            detected_box=detected_box,\n",
    "                                                                            label_ratio_threshold=label_ratio_threshold\n",
    "                                                            )\n",
    "        \n",
    "        ## From original data\n",
    "\n",
    "        output_data = [\n",
    "            ['original', zip(original_boxes, original_image_crop, original_labels)],\n",
    "            ['crop', zip(new_box_coordonates, new_box_data, new_box_labels)]\n",
    "        ]\n",
    "        output_dict = [{\n",
    "            \"filename\":filename,\n",
    "            \"status\":data[0],\n",
    "            \"height\":original_image_shape[0],\n",
    "            \"width\":original_image_shape[1],\n",
    "            \"ratio\":original_image_shape[0]/original_image_shape[1],\n",
    "            \"bbox\":x[0],\n",
    "            \"data\":x[1],\n",
    "            \"label\":x[2]\n",
    "        } for data in output_data for x in data[1]]\n",
    "\n",
    "        return output_dict\n",
    "        \n",
    "\n",
    "    def _filter_box(self, original_image, original_boxes, original_labels, detected_box, label_ratio_threshold=0.7):\n",
    "        \"\"\"Given a box list, return a filtered list and its labels\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        original_image: numpy array of size (h, w, 3) of the original image\n",
    "        original_boxes: list of original box locations in format xmin, xmax, ymin, ymax\n",
    "        original_labels: list integer corresponding of the labels of the original box\n",
    "        detected_box: list of detected box in formay xmin, ymin, xmax, ymax\n",
    "        label_ratio_threshold: threshold of percentage of box intersection for keeping it\n",
    "\n",
    "        Output\n",
    "        ------\n",
    "        Tuple new_box_coordonates, new_box_data, new_box_label :\n",
    "        - new_box_coordonates: list of xmin, ymin, xmax and ymax coordonates\n",
    "        - new_box_data: numpy array of size (h,w) which contains the content of the box\n",
    "        - new_box_label: int of the box class\n",
    "        \"\"\"\n",
    "\n",
    "        # We create a reference matrix, which contains the true labels\n",
    "        label_matrix = np.ones(original_image.shape[0:2])*-1\n",
    "        for original_box, original_label in zip(original_boxes, original_labels):\n",
    "            label_matrix[original_box[2]:original_box[3],original_box[0]:original_box[1]] = original_label\n",
    "\n",
    "        new_box_coordonates = []\n",
    "        new_box_data = []\n",
    "        new_box_label = []\n",
    "\n",
    "        for box in detected_box:\n",
    "            # Create a temporary matrix for working on data\n",
    "            working_matrix = label_matrix[box[1]:box[3],box[0]:box[2]]\n",
    "            if np.max(working_matrix) != -1:      \n",
    "                # Compute the proportion of pixels with a label\n",
    "                label_ratio = (working_matrix != -1).mean()\n",
    "                \n",
    "                if label_ratio > label_ratio_threshold:\n",
    "                    box_label = np.argmax(np.bincount(working_matrix[working_matrix != -1].astype(\"int8\")))-1\n",
    "                    box_data = original_image[box[1]:box[3], box[0]:box[2]]\n",
    "\n",
    "                    new_box_coordonates.append(box)\n",
    "                    new_box_data.append(box_data)\n",
    "                    new_box_label.append(box_label)\n",
    "\n",
    "        return new_box_coordonates, new_box_data, new_box_label\n",
    "\n",
    "    def _write_metadata(self, path):\n",
    "        \"\"\"Write the metadata in a pickle file\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        path: str, path where to write the metadata pickle file\n",
    "        \"\"\"\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"Writting metadata in {path}\")\n",
    "\n",
    "        with open(path, \"wb\") as f:\n",
    "            pickle.dump(self.metadata, f)\n",
    "\n",
    "    def _normalize_data (self, data):\n",
    "        \"\"\"Normalize an image to get a value between 0 and 1\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data: array of size (w,h)\n",
    "        \"\"\"\n",
    "\n",
    "        #data = data / 255.\n",
    "        return data\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"For a given id, return a data\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        idx: int, id of the data to get\n",
    "        \"\"\"\n",
    "\n",
    "        # Getting the metadata\n",
    "        metadata = self.metadata[self.metadata_mask[idx]]\n",
    "\n",
    "        # Loading the data\n",
    "        with open(metadata[\"path\"], \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "            data = self._normalize_data(data)\n",
    "\n",
    "            #data/255.\n",
    "            #data_tensor = torch.tensor(data)\n",
    "\n",
    "        return data, metadata\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = folliclesDataset(\n",
    "    test_loader,\n",
    "    data_augmentation=False,\n",
    "    local_path=\"./datasets/test\",\n",
    "    box_classifier=model._get_box_list,\n",
    "    verbose=True,\n",
    "    order=\"box_ratio\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in test_dataset:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([[[169, 158, 192],\n",
       "         [151, 144, 178],\n",
       "         [134, 131, 174],\n",
       "         ...,\n",
       "         [247, 248, 243],\n",
       "         [246, 246, 244],\n",
       "         [246, 246, 244]],\n",
       " \n",
       "        [[168, 157, 187],\n",
       "         [153, 146, 179],\n",
       "         [141, 137, 174],\n",
       "         ...,\n",
       "         [248, 249, 244],\n",
       "         [247, 247, 245],\n",
       "         [246, 246, 244]],\n",
       " \n",
       "        [[167, 158, 189],\n",
       "         [157, 150, 183],\n",
       "         [151, 147, 184],\n",
       "         ...,\n",
       "         [247, 246, 242],\n",
       "         [245, 245, 243],\n",
       "         [245, 245, 243]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[136, 133, 180],\n",
       "         [134, 132, 179],\n",
       "         [126, 128, 179],\n",
       "         ...,\n",
       "         [248, 253, 246],\n",
       "         [247, 252, 245],\n",
       "         [248, 253, 246]],\n",
       " \n",
       "        [[148, 144, 194],\n",
       "         [149, 147, 196],\n",
       "         [140, 142, 191],\n",
       "         ...,\n",
       "         [249, 251, 246],\n",
       "         [248, 250, 245],\n",
       "         [249, 251, 246]],\n",
       " \n",
       "        [[152, 149, 196],\n",
       "         [154, 151, 196],\n",
       "         [146, 147, 193],\n",
       "         ...,\n",
       "         [248, 250, 245],\n",
       "         [249, 251, 246],\n",
       "         [249, 251, 246]]], dtype=uint8),\n",
       " {'filename': 'D-1M06-5.jpg',\n",
       "  'status': 'crop',\n",
       "  'height': 5431,\n",
       "  'width': 6583,\n",
       "  'ratio': 0.825003797660641,\n",
       "  'bbox': (5388, 1209, 5460, 1260),\n",
       "  'label': -1,\n",
       "  'path': './datasets/test/226.pickle'})"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[len(test_dataset)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6503038246157512"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[1][\"ratio\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'list' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_476283/1214189193.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_dataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_476283/342248185.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"path\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_normalize_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0mdata_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_476283/342248185.py\u001b[0m in \u001b[0;36m_normalize_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    250\u001b[0m         \"\"\"\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'list' and 'float'"
     ]
    }
   ],
   "source": [
    "for x in test_dataloader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = imageDataLoader(test_files, test_label[[\"filename\",\"xmin\",\"xmax\",\"ymin\",\"ymax\",\"label\"]])\n",
    "train_loader = imageDataLoader(train_files, train_label[[\"filename\",\"xmin\",\"xmax\",\"ymin\",\"ymax\",\"label\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.65030382, 0.65030382, 0.65030382, 0.65030382, 0.65030382,\n",
       "       0.65030382, 0.65030382, 0.65030382, 0.65030382, 0.65030382,\n",
       "       0.65030382, 0.65030382, 0.65030382, 0.65030382, 0.65030382,\n",
       "       0.65030382, 0.65030382, 0.65030382, 0.65030382, 0.65030382,\n",
       "       0.65030382, 0.65030382, 0.65030382, 0.65030382, 0.65030382,\n",
       "       0.65030382, 0.65030382, 0.65030382, 0.65030382, 0.65030382,\n",
       "       0.65030382, 0.65030382, 0.65030382, 0.65030382, 0.65030382,\n",
       "       0.65030382, 0.65030382, 0.65030382, 0.65030382, 0.65030382,\n",
       "       0.65030382, 0.65030382, 0.65030382, 0.65030382, 0.65030382,\n",
       "       0.65030382, 0.65030382, 0.65030382, 0.65030382, 0.65030382,\n",
       "       0.65030382, 0.65030382, 0.65030382, 0.65030382, 0.65030382,\n",
       "       0.65030382, 0.65030382, 0.65030382, 0.65030382, 0.65030382,\n",
       "       0.65030382, 0.65030382, 0.65030382, 0.65030382, 0.65030382,\n",
       "       0.65030382, 0.65030382, 0.65030382, 0.65030382, 0.65030382,\n",
       "       0.65030382, 0.65030382, 0.65030382, 0.65030382, 0.65030382,\n",
       "       0.65030382, 0.65030382, 0.65030382, 0.65030382, 0.65030382,\n",
       "       0.65030382, 0.65030382, 0.65030382, 0.65030382, 0.65030382,\n",
       "       0.65030382, 0.65030382, 0.65030382, 0.65030382, 0.65030382,\n",
       "       0.65030382, 0.65030382, 0.65030382, 0.65030382, 0.65030382,\n",
       "       0.65030382, 0.65030382, 0.65030382, 0.65030382, 0.65030382,\n",
       "       0.65030382, 0.65030382, 0.65030382, 0.65030382, 0.65030382,\n",
       "       0.65030382, 0.65030382, 0.65030382, 0.65030382, 0.65030382,\n",
       "       0.65030382, 0.65030382, 0.65030382, 0.65030382, 0.65030382,\n",
       "       0.65030382, 0.65030382, 0.65030382, 0.65030382, 0.65030382,\n",
       "       0.65030382, 0.65030382, 0.65030382, 0.65030382, 0.65030382,\n",
       "       0.65030382, 0.65030382, 0.65030382, 0.65030382, 0.65030382,\n",
       "       0.65030382, 0.65030382, 0.65030382, 0.65030382, 0.65030382,\n",
       "       0.65030382, 0.65030382, 0.65030382, 0.65030382, 0.65030382,\n",
       "       0.65030382, 0.65030382, 0.65030382, 0.65030382, 0.65030382,\n",
       "       0.65030382, 0.65030382, 0.65030382, 0.65030382, 0.65030382,\n",
       "       0.65030382, 0.65030382, 0.65030382, 0.65030382, 0.65030382,\n",
       "       0.65030382, 0.65030382, 0.65030382, 0.65030382, 0.65030382,\n",
       "       0.65030382, 0.65030382, 0.65030382, 0.65030382, 0.65030382,\n",
       "       0.65030382, 0.67331065, 0.67331065, 0.67331065, 0.67331065,\n",
       "       0.67331065, 0.67331065, 0.67331065, 0.67331065, 0.67331065,\n",
       "       0.67331065, 0.67331065, 0.67331065, 0.67331065, 0.67331065,\n",
       "       0.67331065, 0.67331065, 0.67331065, 0.67331065, 0.67331065,\n",
       "       0.67331065, 0.67331065, 0.67331065, 0.67331065, 0.67331065,\n",
       "       0.67331065, 0.67331065, 0.67331065, 0.67331065, 0.67331065,\n",
       "       0.74545944, 0.74545944, 0.74545944, 0.74545944, 0.74545944,\n",
       "       0.74545944, 0.74545944, 0.74545944, 0.74545944, 0.74545944,\n",
       "       0.74545944, 0.74545944, 0.74545944, 0.74545944, 0.74545944,\n",
       "       0.74545944, 0.74545944, 0.74545944, 0.74545944, 0.74545944,\n",
       "       0.74545944, 0.74545944, 0.74545944, 0.74545944, 0.74545944,\n",
       "       0.74545944, 0.74545944, 0.74545944, 0.74545944, 0.74545944,\n",
       "       0.74545944, 0.74545944, 0.74545944, 0.74545944, 0.74545944,\n",
       "       0.74545944, 0.74545944, 0.74545944, 0.74545944, 0.79912725,\n",
       "       0.79912725, 0.79912725, 0.79912725, 0.79912725, 0.79912725,\n",
       "       0.79912725, 0.79912725, 0.79912725, 0.79912725, 0.79912725,\n",
       "       0.79912725, 0.79912725, 0.79912725, 0.79912725, 0.79912725,\n",
       "       0.79912725, 0.79912725, 0.79912725, 0.8250038 , 0.8250038 ,\n",
       "       0.8250038 , 0.8250038 , 0.8250038 , 0.8250038 , 0.8250038 ,\n",
       "       0.8250038 , 0.8250038 , 0.8250038 , 0.8250038 , 0.8250038 ,\n",
       "       0.8250038 , 0.8250038 , 0.8250038 , 0.8250038 , 0.8250038 ,\n",
       "       0.8250038 , 0.8250038 , 0.8250038 , 0.8250038 , 0.8250038 ,\n",
       "       0.8250038 , 0.8250038 ])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratio_list = [x[\"ratio\"] for x in test_dataset.metadata]\n",
    "np.array(ratio_list)[np.argsort(ratio_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = folliclesDataset(\n",
    "    train_loader,\n",
    "    data_augmentation=True,\n",
    "    local_path=\"./datasets/train\",\n",
    "    box_classifier=model._get_box_list,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1082"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset.metadata)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "28b293e0c0671e44c7281dde6399c7c7419d3faca031d22494da8635907ada72"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
